{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Forward propagation in RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.27192674 0.22924076 0.22972795 0.26910455]\n",
      " [0.25702217 0.22563602 0.24269542 0.27464638]\n",
      " [0.24756057 0.26493581 0.26474527 0.22275835]\n",
      " [0.24541756 0.24705111 0.26236208 0.24516926]\n",
      " [0.27761669 0.21655925 0.2533894  0.25243466]\n",
      " [0.25116786 0.25338565 0.27042823 0.22501825]\n",
      " [0.26676378 0.23981495 0.22719182 0.26622945]\n",
      " [0.26214189 0.22619336 0.24652592 0.26513883]\n",
      " [0.24033609 0.25967042 0.24913256 0.25086093]\n",
      " [0.22278486 0.26560348 0.25935337 0.25225829]\n",
      " [0.28136741 0.21578339 0.23724153 0.26560767]\n",
      " [0.26197608 0.22801507 0.25498731 0.25502154]\n",
      " [0.24819485 0.2338042  0.27842576 0.23957518]\n",
      " [0.26098481 0.23785486 0.25415253 0.24700779]\n",
      " [0.23244361 0.26361104 0.26328685 0.2406585 ]\n",
      " [0.2563248  0.25760124 0.25803609 0.22803787]\n",
      " [0.28202029 0.23147273 0.24405827 0.24244872]\n",
      " [0.27930991 0.23219813 0.23595022 0.25254173]\n",
      " [0.23615503 0.27358818 0.26187602 0.22838077]\n",
      " [0.2540332  0.25893927 0.25979356 0.22723397]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "input_dim = 16 \n",
    "hidden_dim = 16\n",
    "output_dim=4\n",
    "\n",
    "\n",
    "# 1Initialize all the weights, , , and , by randomly drawing from the uniform distribution:\n",
    "U = np.random.uniform(-np.sqrt(1.0 / input_dim), np.sqrt(1.0 / input_dim),(hidden_dim, input_dim))\n",
    "W = np.random.uniform(-np.sqrt(1.0 / hidden_dim), np.sqrt(1.0 / hidden_dim),(hidden_dim, hidden_dim))\n",
    "V = np.random.uniform(-np.sqrt(1.0 / hidden_dim), np.sqrt(1.0 / hidden_dim),(output_dim, hidden_dim))\n",
    "\n",
    "\n",
    "# the input sequence\n",
    "x = np.random.randint(0, input_dim, size=20)\n",
    "\n",
    "# 2. Define the number of time steps, which will be the length of our inputsequence, :\n",
    "num_time_steps = len(x)\n",
    "\n",
    "# 3. Define the hidden state:\n",
    "hidden_state = np.zeros((num_time_steps + 1, hidden_dim))\n",
    "\n",
    "# 4. Initialize the initial hidden state,hinit , with zeros:\n",
    "hidden_state[-1] = np.zeros(hidden_dim)\n",
    "\n",
    "\n",
    "# Initialize the output\n",
    "YHat = np.zeros((num_time_steps, output_dim))\n",
    "\n",
    "# 6. For every time step, we perform the following:\n",
    "for t in np.arange(num_time_steps):\n",
    "    # h_t = tanh(UX + Wh_{t-1})\n",
    "    hidden_state[t] = np.tanh(U[:, x[t]] + W.dot(hidden_state[t - 1]))\n",
    "    # yhat_t = softmax(vh)\n",
    "    YHat[t] = softmax(V.dot(hidden_state[t]))\n",
    "    \n",
    "print(YHat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Lyrics Generation with RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/songdata.csv')\n",
    "# Let's see what we have in our dataset:\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data: 57650 \n",
      "\n",
      "Number of unique artists: 643\n",
      "\n",
      "artist\n",
      "Donna Summer        191\n",
      "Gordon Lightfoot    189\n",
      "Bob Dylan           188\n",
      "George Strait       188\n",
      "Loretta Lynn        187\n",
      "Cher                187\n",
      "Alabama             187\n",
      "Reba Mcentire       187\n",
      "Chaka Khan          186\n",
      "Dean Martin         186\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Average number of songs per artist: 89.65785381026438\n"
     ]
    }
   ],
   "source": [
    "# The preceding code generates the following output:\n",
    "# Our dataset consists of about 57,650 song lyrics:\n",
    "print(f\"Total Data: {df.shape[0]} \\n\" )\n",
    "\n",
    "# We have song lyrics from about 643 artists:\n",
    "print(f\"Number of unique artists: {len(df['artist'].unique())}\\n\")\n",
    "\n",
    "# The number of songs from each artist is shown as follows:\n",
    "print(df['artist'].value_counts()[:10])\n",
    "\n",
    "\n",
    "print(f\"\\nAverage number of songs per artist: {df['artist'].value_counts().values.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look at her face, it's a wonderful face  \n",
      "And it means something special to me  \n",
      "Look at the way that she smiles when she sees me  \n",
      "How lucky can one fellow be?  \n",
      "  \n",
      "She's just my kind of girl, she makes me feel fine  \n",
      "Who could ever believe that she could be mine?  \n",
      "She's just my kind of girl, without her I'm blue  \n",
      "And if she ever leaves me what could I do, what co\n",
      "\n",
      "68\n",
      "s\n",
      "[0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "data = \", \".join(df[\"text\"])\n",
    "print(data[:369],end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "print(char_to_ix[\"s\"])\n",
    "\n",
    "print(ix_to_char[68])\n",
    "\n",
    "vocabSize = 7\n",
    "char_index = 4\n",
    "print(np.eye(vocabSize)[char_index])\n",
    "\n",
    "def one_hot_encoder(index):\n",
    "    return np.eye(vocab_size)[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Defining Network Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "# Define the number of units in the hidden layer\n",
    "hidden_size = 100\n",
    "\n",
    "# Define the length of the input and output sequence\n",
    "seq_length = 25\n",
    "\n",
    "# Define the learning rate for gradient descent\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# Set the seed value\n",
    "seed_value = 42\n",
    "tf.random.set_seed(seed_value)\n",
    "random.seed(seed_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Defining Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "# 1. The placeholders for the input and output is defined as:\n",
    "inputs = tf.compat.v1.placeholder(shape=[None, vocab_size],dtype=tf.float32,name=\"inputs\")\n",
    "targets = tf.compat.v1.placeholder(shape=[None, vocab_size], dtype=tf.float32,name=\"targets\")\n",
    "# 2. Define the placeholder for the initial hidden state:\n",
    "init_state = tf.compat.v1.placeholder(shape=[1, hidden_size], dtype=tf.float32,name=\"state\")\n",
    "# 3. Define an initializer for initializing the weights of the RNN:\n",
    "initializer = tf.random_normal_initializer(stddev=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Defining forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the forward propagation within a variable scope named \"RNN\"\n",
    "with tf.compat.v1.variable_scope(\"RNN\") as scope:\n",
    "    h_t = init_state  # Initialize the hidden state as the initial state\n",
    "    y_hat = []  # To store the outputs at each time step\n",
    "\n",
    "    # Iterate through each time step of the sequence\n",
    "    for t, x_t in enumerate(tf.split(inputs, seq_length, axis=0)):\n",
    "        if t > 0:\n",
    "            scope.reuse_variables()  # Reuse variables after the first time step\n",
    "\n",
    "        # Define weight matrices for input-to-hidden, hidden-to-hidden, and hidden-to-output layers\n",
    "        U = tf.compat.v1.get_variable(\"U\", [vocab_size, hidden_size], initializer=initializer)\n",
    "        W = tf.compat.v1.get_variable(\"W\", [hidden_size, hidden_size], initializer=initializer)\n",
    "        V = tf.compat.v1.get_variable(\"V\", [hidden_size, vocab_size], initializer=initializer)\n",
    "\n",
    "        # Define biases for the hidden and output layers\n",
    "        bh = tf.compat.v1.get_variable(\"bh\", [hidden_size], initializer=initializer)\n",
    "        by = tf.compat.v1.get_variable(\"by\", [vocab_size], initializer=initializer)\n",
    "\n",
    "        # Calculate the new hidden state using the input x_t and the previous hidden state h_t\n",
    "        h_t = tf.tanh(tf.matmul(x_t, U) + tf.matmul(h_t, W) + bh)\n",
    "\n",
    "        # Calculate the output y_hat_t for the current time step\n",
    "        y_hat_t = tf.matmul(h_t, V) + by\n",
    "        y_hat.append(y_hat_t)  # Append the output to the y_hat list\n",
    "\n",
    "# Apply softmax to the final output to get probabilities\n",
    "output_softmax = tf.nn.softmax(y_hat[-1])\n",
    "outputs = tf.concat(y_hat, axis=0)\n",
    "\n",
    "# Compute the cross-entropy loss between the outputs and the targets\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=targets, logits=outputs))\n",
    "\n",
    "# Store the final hidden state of the RNN, which can be used for predictions or further processing\n",
    "hprev = h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPTT\n",
    "\n",
    "# Initialize the Adam optimizer\n",
    "minimizer = tf.compat.v1.train.AdamOptimizer()\n",
    "\n",
    "# Compute the gradients of the loss with the Adam optimizer\n",
    "gradients = minimizer.compute_gradients(loss)\n",
    "\n",
    "# Set the threshold for gradient clipping\n",
    "threshold = tf.constant(5.0, name=\"grad_clipping\")\n",
    "\n",
    "# Clip the gradients that exceed the threshold and bring them within the range\n",
    "clipped_gradients = []\n",
    "for grad, var in gradients:\n",
    "    if grad is not None:  # Check if the gradient is not None\n",
    "        clipped_grad = tf.clip_by_value(grad, -threshold, threshold)\n",
    "        clipped_gradients.append((clipped_grad, var))\n",
    "\n",
    "# Apply the clipped gradients to update the model's variables\n",
    "updated_gradients = minimizer.apply_gradients(clipped_gradients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Start generating songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence:  Look at her face, it's a \n",
      "Output Sentence:  ook at her face, it's a w\n",
      "\n",
      "\n",
      "After 0 iterations\n",
      "\n",
      " \"CtmYG.CCkY'j4GR9yXIrPicosoDp].5[8L jJa]Zex02xCTbVwawO26:po24BQn9mtnB  Zb5:[Job!u3tqNqaK?9LRUz!dL:Ey7gZO7lKZ -seurw Fcn)X!G(G!:kgpI(DyT-Q0p8Mw5pQjJO-w'TX1] Gz]d7sEr?O[tp\"FBmX1lC'SkZu1j)Bd.2,EpE-4.1\"o1y3G13(oBJCSgDn?aan]'AL(IyTeK,LyEKqnROM1-eK)R7xRl\"Cc:CG:HIuh1eE!t4p1f?)kOF6z0g4l,owsliOQ:\n",
      "'K\n",
      "yDBKpn0uu8LJ7nu?8aXAzSL?eMOSVvUeD7-4x.WwK[op:MlnZPG5.3-BcHag?:48l0vAo4?LF0[65ao,Z,lk7[4EUD\n",
      "Dw'Wzkx\n",
      "JLwgX8)0 f2Pe (YUV9u65!-m-w(7O[pd(76!kSo[6.sZkT2n\n",
      "8?Q'tWNwN:t6vBX'jHFKPw1pn-Lok(L7Tj3iboYwlV0)7Gqz3wtGDSw9rwV \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "After 50000 iterations\n",
      "\n",
      "  heep, you have it and here  \n",
      "Foold, me opdir  \n",
      "Non't, dodn everying  \n",
      "Canbe or  \n",
      "[flling ohd  \n",
      "[Thaigh... (Malh... HBadony-here what me it.. ,I red hear Brathe! ridal allice  \n",
      "[.  \n",
      "[Mrs.]. Trie]  \n",
      "True prownathiow!!'Baigh.  \n",
      "[Mnter  \n",
      "[And eldaga!\n",
      "\n",
      ", I don't tro khat me and of life  \n",
      "Rand mednow I needs.  \n",
      "[Cjuive  \n",
      "Change Losomeye onror derassy, chroks\n",
      "\n",
      ", Ges a sasever hrazes me  \n",
      "Lise is oninus] up hance:]  \n",
      "[Nctuers a baby  \n",
      "[Edtuedps lfigh.  \n",
      "[yotherd can arver me and whenid a where your a c \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "After 100000 iterations\n",
      "\n",
      " knewer  \n",
      "All I my)  \n",
      "And deee, sworsed, ricks fromn tis cenight  \n",
      "Let you've my streltrelt to let I'd to not.  \n",
      "Hane eld, I'm just lakes metiong is just  \n",
      "Reayon  \n",
      "You  \n",
      "asm my wrick, can't I wathout you  \n",
      "  \n",
      "So gets not my over turn to dredwed his I not down, down to me  \n",
      "A bobngep I'm doing  \n",
      "Thy grow aud to pactootely're me everye dose be werks pight tod  \n",
      "Ret  \n",
      "  \n",
      "Take me  \n",
      "  \n",
      "Heake  \n",
      "(I deinctown  \n",
      "That it thhre to keep tho lasted to yeah I ayt to facklight still mustand of there youll I d- \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "After 150000 iterations\n",
      "\n",
      "  be fought when you lest out so you be so long  \n",
      "  \n",
      "Spot that you sheat yeah  \n",
      "I'm a with yeah  \n",
      "You waste evwry the way yeah  \n",
      "Doinge dog't yeah you know and you the oh, I want ol catthers right the's just it everytony let're go  \n",
      "Is a lot reayen shin your dogrions night  \n",
      "Everytain oh oh yeah oh yeah yeah,  \n",
      "  \n",
      "I wantly it you see I'd get  \n",
      "  \n",
      "(It you fool everything's not light  \n",
      "Oh yeah yeah  \n",
      "Everything wascem  \n",
      "So leave the'll one of yeah no shootr  \n",
      "  \n",
      "Well  \n",
      "  \n",
      "Yeurngold yeah  \n",
      "For guet  \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "After 200000 iterations\n",
      "\n",
      " ee  \n",
      "Can't soon to want tried  \n",
      "Cannave that I want to be thing I need do long and of fine, I can sometime I know  \n",
      "  \n",
      "Wounge a night I calry baby to stind we been a wornd  \n",
      "This  \n",
      "(baby I gone  \n",
      "I'm gonna make a role be time to be baby, to think I bronged  \n",
      "Tamile has your love  \n",
      "Baby, be and i tuill you sathing you fash I just let you think of m oh like dost for my wave you life, semomind  \n",
      "Swite on ya \n",
      "Cworkum to be love what you regin so him you cryst turn up in pright: can't gooe a smilha o \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialize TensorFlow session and variables\n",
    "sess = tf.compat.v1.Session()\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "# Dataset and pointer initialization\n",
    "pointer = 0\n",
    "\n",
    "# Define input and output data\n",
    "input_sentence = data[pointer:pointer + seq_length]\n",
    "print(\"Input Sentence: \",input_sentence)\n",
    "\n",
    "output_sentence = data[pointer + 1:pointer + seq_length + 1]\n",
    "print(\"Output Sentence: \",output_sentence)\n",
    "\n",
    "\n",
    "\n",
    "sample_length = 500\n",
    "\n",
    "for iteration in range(500001):\n",
    "    if pointer + seq_length+1 >= len(data) or iteration == 0:\n",
    "        hprev_val = np.zeros([1, hidden_size])\n",
    "        pointer = 0\n",
    "    \n",
    "    input_sentence = data[pointer:pointer + seq_length]\n",
    "    output_sentence = data[pointer + 1:pointer + seq_length + 1]\n",
    "\n",
    "    # Convert sentences to indices\n",
    "    input_indices = [char_to_ix[ch] for ch in input_sentence]\n",
    "    target_indices = [char_to_ix[ch] for ch in output_sentence]\n",
    "\n",
    "    # Convert indices to one-hot vectors\n",
    "    input_vector = one_hot_encoder(input_indices)\n",
    "    target_vector = one_hot_encoder(target_indices)\n",
    "        \n",
    "    hprev_val, loss_val, _ = sess.run(\n",
    "        [hprev, loss, updated_gradients],\n",
    "        feed_dict={\n",
    "            inputs: input_vector,\n",
    "            targets: target_vector,\n",
    "            init_state: hprev_val\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Generate song lyrics every 50,000 iterations\n",
    "    if iteration % 50000 == 0:\n",
    "        random_index = random.randint(0, len(data) - seq_length)\n",
    "        sample_input_sent = data[random_index:random_index + seq_length]\n",
    "        sample_input_indices = [char_to_ix[ch] for ch in sample_input_sent]\n",
    "        sample_prev_state_val = np.copy(hprev_val)\n",
    "        predicted_indices = []\n",
    "\n",
    "        for t in range(sample_length):\n",
    "            sample_input_vector = one_hot_encoder(sample_input_indices)\n",
    "            probs_dist, sample_prev_state_val = sess.run(\n",
    "                [output_softmax, hprev],\n",
    "                feed_dict={\n",
    "                    inputs: sample_input_vector,\n",
    "                    init_state: sample_prev_state_val\n",
    "                }\n",
    "            )\n",
    "\n",
    "            ix = np.random.choice(range(len(char_to_ix)), p=probs_dist.ravel())\n",
    "            sample_input_indices = sample_input_indices[1:] + [ix]\n",
    "            predicted_indices.append(ix)\n",
    "\n",
    "        predicted_chars = [ix_to_char[ix] for ix in predicted_indices]\n",
    "        text = ''.join(predicted_chars)\n",
    "        print('\\n')\n",
    "        print(f'After {iteration} iterations')\n",
    "        print('\\n', text, '\\n')\n",
    "        print('-' * 115)\n",
    "\n",
    "    pointer += seq_length\n",
    "    iteration += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
